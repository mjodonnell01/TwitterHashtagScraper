{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a8086e-b650-4003-bb25-7665147e0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC \n",
    "#import time\n",
    "#import re\n",
    "#from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c53eacb0-aea2-42de-84c0-3ca8645b0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_hashtag = ['TruckersForTrump', 'Republican', 'Trump', 'FakeNews', 'Conspiracy', 'LetsGoBrandon', 'Hoax', 'Conspiracy', 'FlatEarth', 'Guns', '2A', 'Rigged', 'RightWingMedia', 'RightWing', 'ProLife', 'Jesus', 'God', 'Christianity', 'Church', 'Christian', 'Divorced', 'Drunk', 'ExWife']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60e1f3a6-8465-4d83-9004-7981fc628e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_hashtag = ['Trump', 'Biden', 'Politics', 'Ukraine', 'Russia', 'Israel', 'Gaza', 'Prolife', 'Jesus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f21257d-db3d-4c99-8c1f-13d40924fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_tweets = ['I miss you Sheila','Please come back baby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "693eac7a-d397-4a0c-bc0d-e70b51f9c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8e120-b8c9-4ae7-a03e-7a7ede8eb7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags scraped: 1 / 8\n",
      "Tweets scraped: 262\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n",
      "Hashtags scraped: 2 / 8\n",
      "Tweets scraped: 529\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n",
      "Hashtags scraped: 3 / 8\n",
      "Tweets scraped: 758\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n",
      "Hashtags scraped: 4 / 8\n",
      "Tweets scraped: 982\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n",
      "Hashtags scraped: 5 / 8\n",
      "Tweets scraped: 1192\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n",
      "Hashtags scraped: 6 / 8\n",
      "Tweets scraped: 1413\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n",
      "Hashtags scraped: 7 / 8\n",
      "Tweets scraped: 1597\n",
      "Time remaining for next scrape: 1 secondsss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "scrape_hashtag = ['Trump', 'Biden', 'Politics', 'Ukraine', 'Russia', 'Israel', 'Gaza', 'Jesus']\n",
    "complete_tweets = ['I miss you Sheila','Please come back baby']\n",
    "counter = 0\n",
    "\n",
    "for each_hashtag in scrape_hashtag:\n",
    "    #Launches chrome\n",
    "    driver=webdriver.Chrome()\n",
    "    #Goes to Twitter Login Page\n",
    "    driver.get('https://twitter.com/i/flow/login')\n",
    "    time.sleep(5)\n",
    "    #Enters username\n",
    "    element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//input[@class=\"r-30o5oe r-1dz5y72 r-13qz1uu r-1niwhzg r-17gur6a r-1yadl64 r-deolkf r-homxoj r-poiln3 r-7cikom r-1ny4l3l r-t60dpp r-fdjqy7\"]')))\n",
    "    element.send_keys(\"IHeartTrump2024\" + Keys.ENTER) \n",
    "    time.sleep(5)\n",
    "    #Enters password and logs in\n",
    "    password = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//input[@class=\"r-30o5oe r-1dz5y72 r-13qz1uu r-1niwhzg r-17gur6a r-1yadl64 r-deolkf r-homxoj r-poiln3 r-7cikom r-1ny4l3l r-t60dpp r-fdjqy7\"]')))\n",
    "    password.send_keys('FacialToner0101!' + Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "    #sets empty list\n",
    "    pagesource = ''\n",
    "    #searches the hashtag\n",
    "    search_hashtag = driver.get('https://twitter.com/search?q=%23' + str(each_hashtag) + '&src=typed_query&f=live')\n",
    "    #scrolls to the bottom of the page 25 times while scraping the page each time\n",
    "    for i in range(0,25):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        pagesource += driver.page_source\n",
    "    time.sleep(5)\n",
    "    #quits the chrome page\n",
    "    driver.quit()\n",
    "    #Parses the collected HTML that was gathered\n",
    "    bs = BeautifulSoup(pagesource,'html.parser')\n",
    "    #creates empty tweet list\n",
    "    tweet_list = []\n",
    "    for tweets in bs.find_all('div', class_='css-1rynq56 r-8akbws r-krxsd3 r-dnmrzs r-1udh08x r-bcqeeo r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41 r-bnwqim'):\n",
    "        tweet_content = tweets.text\n",
    "        tweet_list.append(tweet_content)\n",
    "    filtered_tweets = [s.replace('\\n', '') for s in tweet_list]\n",
    "    drop_dupes = list(set(filtered_tweets))\n",
    "    #tweets.append(drop_dupes)\n",
    "    complete_tweets += drop_dupes\n",
    "    counter += 1\n",
    "    print('Hashtags scraped: ' + str(counter) + ' / ' + str(len(scrape_hashtag)) + '\\n', end='\\r')\n",
    "    print('Tweets scraped: ' + str(len(complete_tweets)) + '\\n', end='\\r')\n",
    "    #print('Sleeping for 10 minutes, will scrape again soon\\n')\n",
    "    #time.sleep(600)\n",
    "    def countdown(timer):\n",
    "        for i in range(timer, 0, -1):\n",
    "            #print(f\"Time remaining: {i} seconds\", end='\\r')\n",
    "            print('Time remaining for next scrape: ' + str(i) + ' seconds', end='\\r')\n",
    "            time.sleep(1)\n",
    "    # Set the countdown timer (in seconds)\n",
    "    timer_seconds = 600\n",
    "    if each_hashtag == scrape_hashtag[-1]:\n",
    "        print('\\nScraping finished\\n')\n",
    "    else:\n",
    "        countdown(timer_seconds)\n",
    "        print('\\n')\n",
    "\n",
    "def remove_non_english(input_string):\n",
    "    # Using regex to remove non-English characters\n",
    "    english_only = re.sub(r'[^a-zA-Z0-9\\s#@]', '', input_string)\n",
    "    return english_only\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http[s]?://\\S+', '', text)\n",
    "# Applying the function to each string in the list\n",
    "filtered_list01 = [remove_non_english(s) for s in complete_tweets]\n",
    "filtered_list02 = [remove_links(text) for text in filtered_list01]\n",
    "print('Tweets ready for export\\n')\n",
    "date = str(datetime.today().strftime('%m%d%Y'))\n",
    "file_path = 'tweets_' + date +'.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for text in filtered_list02:\n",
    "        file.write(text + '\\n')\n",
    "print('\\nFile created\\nScraping is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb28b352-b959-4435-a8d7-dad4a5668a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_non_english(input_string):\n",
    "    # Using regex to remove non-English characters\n",
    "#    english_only = re.sub(r'[^a-zA-Z0-9\\s#]', '', input_string)\n",
    "#    return english_only\n",
    "#def remove_links(text):\n",
    "#    return re.sub(r'http[s]?://\\S+', '', text)\n",
    "# Applying the function to each string in the list\n",
    "#filtered_list01 = [remove_non_english(s) for s in complete_tweets]\n",
    "#filtered_list02 = [remove_links(text) for text in filtered_list01]\n",
    "#print('Tweets ready for export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0ae864-1c5c-46cf-883d-61bceea92dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date = str(datetime.today().strftime('%m%d%Y'))\n",
    "#file_path = 'tweets_' + date +'.txt'\n",
    "\n",
    "#with open(file_path, 'w') as file:\n",
    "#    for text in filtered_list02:\n",
    "#        file.write(text + '\\n')\n",
    "#print('\\nFile created\\nScraping is finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
